# Implementing Channel Interruption

This document outlines how to implement channel interruption for long-running processes (like AI streaming) using the existing WebSocket hub infrastructure. The core idea is to leverage the pub/sub mechanism to send an interruption signal from a client to a specific server-side process handling the long-running task.

## Overview

1.  **Client Initiates Interruption:** The client sends a specific message (`"action": "interrupt"`) targeting the channel associated with the long-running task.
2.  **Hub Relays the Message:** The `client.go` code recognizes this action and prepares a `Broadcast` message with a distinct event type (e.g., `"system:interrupt"`). The `hub.go` broadcasts this message via the message broker.
3.  **Server Handler Listens:** The server-side code that initiated and manages the long-running task (e.g., the ChatGPT streaming handler) must *also* be subscribed to the *same channel* via the message broker.
4.  **Handler Processes Interruption:** When the handler receives the `"system:interrupt"` message, it identifies if the interruption applies to its current task (e.g., using a unique task ID) and cancels the ongoing operation (e.g., using `context.Context` cancellation).

## Implementation Steps

### 1. Define the Interruption Message (Client-Side)

The client needs to send a JSON message indicating the intent to interrupt a task on a specific channel. Including an `operation_id` is crucial if multiple long-running tasks could potentially exist on the same channel simultaneously.

  ```json
  {
    "action": "interrupt",
    "channel": "your-long-running-task-channel",
    "data": {
      "operation_id": "unique-task-identifier-123" 
    }
  }
  ```

### 2. Handle Interruption Action in `client.go`

Modify the `readPump` function in `client.go` to handle the `"interrupt"` action. It should create a `Broadcast` message intended for the server-side handler, not necessarily other clients.

  ```go
  // In client.go, inside the readPump function's main loop:

  // ... existing code ...

  case "interrupt":
    // Extract operation ID if provided
    var operationID string
    if dataMap, ok := dataRaw.(map[string]interface{}); ok {
        operationID, _ = dataMap["operation_id"].(string)
    }

    // Prepare interruption data payload
    interruptData := map[string]interface{}{
        "event": "system:interrupt", // Specific event type for interruption
        "operation_id": operationID,   // Pass the ID along
    }
    interruptPayload, err := json.Marshal(interruptData)
    if err != nil {
        log.Printf("[ReadPump] Failed to marshal interrupt data: %v", err)
        continue
    }

    // Send as a broadcast on the target channel
    // The server-side handler for this channel needs to listen for this.
    hubBroadcastChannel <- Broadcast{
        ChannelName: channel,
        Event:       "system:interrupt", // Use the specific event
        Data:        interruptPayload,
        SenderID:    c.UserID,          // Identify who requested interruption
        Timestamp:   time.Now(),
        // MessageID will be generated by the hub's handleBroadcast
    }


  // ... existing case "whisper": ...

  ```

### 3. Server-Side Handler Logic (Conceptual)

The key change is *not* within `hub.go` itself, but in how you structure the code that *handles* the long-running task (e.g., the HTTP handler or service that starts the ChatGPT stream).

This handler needs to:

*   **Generate a Unique ID:** When starting the long-running task, create a unique `operation_id`.
*   **Use Cancellable Context:** Start the task goroutine with a `context.Context` that can be cancelled.
*   **Subscribe to Interruptions:** The handler (or the goroutine) must subscribe to the *same channel* via the `MessageBroker` that the task is associated with. It should filter for the `"system:interrupt"` event.
*   **Listen and Cancel:** In a separate goroutine or select statement, listen for messages from the broker subscription. If an interruption message arrives matching the `operation_id`, cancel the context passed to the long-running task goroutine.

  ```go
  package main

  import (
      "context"
      "encoding/json"
      "fmt"
      "log"
      "time"
      "yourapp/ws" // Import your WebSocket package
      // other necessary imports: net/http, etc.
      "github.com/google/uuid" 
  )

  // Assume 'messageBroker' and 'hub' are initialized instances 
  var messageBroker ws.MessageBroker 
  var hub ws.Hub 

  // Example HTTP handler starting a long task
  func handleStartLongTask(w http.ResponseWriter, r *http.Request) {
      // 1. Get user ID, determine channel name etc.
      userID := "user-123" // from auth
      channelName := "user-task-channel-" + userID
      operationID := uuid.NewString() // Unique ID for this task

      // 2. Create cancellable context
      ctx, cancel := context.WithCancel(context.Background())
      defer cancel() // Ensure cancellation happens eventually

      // 3. Subscribe to the channel for interruption signals *for this specific task*
      //    (This subscription is internal, not a WebSocket client)
      brokerCtx, brokerCancel := context.WithCancel(context.Background())
      defer brokerCancel() // Clean up broker subscription

      msgCh, err := messageBroker.Subscribe(brokerCtx, channelName)
      if err != nil {
          log.Printf("Failed to subscribe for interruption: %v", err)
          http.Error(w, "Failed to set up task", http.StatusInternalServerError)
          return
      }

      // 4. Goroutine to listen for interruption signals
      go func() {
          log.Printf("Listening for interruptions on %s for op %s", channelName, operationID)
          for {
              select {
              case <-brokerCtx.Done(): // Stop listening if context is cancelled
                  log.Printf("Interruption listener stopped for op %s", operationID)
                  return
              case rawMsg := <-msgCh:
                  var broadcast ws.Broadcast
                  if err := json.Unmarshal(rawMsg, &broadcast); err == nil {
                      // Check if it's our specific interruption event
                      if broadcast.Event == "system:interrupt" {
                           var interruptData map[string]interface{}
                           if err := json.Unmarshal(broadcast.Data, &interruptData); err == nil {
                              receivedOpID, _ := interruptData["operation_id"].(string)
                              // Check if the interrupt is for *this* specific operation
                              if receivedOpID == operationID {
                                  log.Printf("Interruption signal received for operation %s on channel %s. Cancelling task.", operationID, channelName)
                                  cancel() // <-- Cancel the main task context
                                  return   // Stop listening
                              }
                           }
                      }
                  }
              }
          }
      }()


      // 5. Start the actual long-running task in its own goroutine
      go func() {
          log.Printf("Starting long task %s on channel %s", operationID, channelName)
          err := performLongRunningTask(ctx, channelName, userID, operationID)
          if err != nil {
             // Handle task errors (potentially notify client)
             log.Printf("Task %s failed: %v", operationID, err)
          } else {
             log.Printf("Task %s completed.", operationID)
          }
          // Task finished or cancelled, clean up the broker listener too
          brokerCancel() 
      }()

      // Respond to the initial HTTP request immediately
      w.WriteHeader(http.StatusOK)
      fmt.Fprintf(w, "Task %s started on channel %s", operationID, channelName)
  }


  // Placeholder for the actual task logic
  func performLongRunningTask(ctx context.Context, channelName, userID, operationID string) error {
      ticker := time.NewTicker(2 * time.Second)
      defer ticker.Stop()
      count := 0

      for {
          select {
          case <-ctx.Done(): // Check if the context has been cancelled
              log.Printf("[Task %s] Context cancelled. Stopping.", operationID)
              // Send a final "cancelled" message if needed
              // hub.GetBroadcastChannel() <- ws.Broadcast{ ... } 
              return ctx.Err() // Return context error (cancelled or deadline exceeded)
          
          case <-ticker.C:
              count++
              // Simulate work and sending progress
              progressData := map[string]interface{}{"progress": count * 10, "operation_id": operationID}
              payload, _ := json.Marshal(progressData)
              
              log.Printf("[Task %s] Sending progress %d", operationID, count)
              hub.GetBroadcastChannel() <- ws.Broadcast{
                  ChannelName: channelName,
                  Event:       "task:progress",
                  Data:        payload,
                  SenderID:    "system", // Task sender
                  Timestamp:   time.Now(),
              }

              if count >= 10 { // Simulate task completion
                  log.Printf("[Task %s] Task finished naturally.", operationID)
                  // Send a final "completed" message
                  // hub.GetBroadcastChannel() <- ws.Broadcast{ ... } 
                  return nil 
              }
          }
      }
  }

  ```

### 4. Hub (`hub.go`) - No Direct Changes Needed

The existing `handleBroadcast` function in `hub.go` is sufficient. It receives the `Broadcast` message (including the `"system:interrupt"` event) from `client.go` and publishes it to the message broker. The server-side handler, subscribed via the broker, will receive it.

## Summary

This approach uses the existing message broker as the communication backbone for interruption signals. The `Client` sends a specific action, the `Hub` broadcasts it, and the responsible server-side handler listens for and acts upon this broadcasted signal using context cancellation. This minimizes changes to the core `Hub` logic and keeps task management responsibility within the specific handler that owns the task.